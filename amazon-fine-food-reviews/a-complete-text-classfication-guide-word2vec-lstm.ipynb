{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification on Amazon Fine Food Dataset with Google Word2Vec Word Embeddings in Gensim and training using LSTM In Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTING THE MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore the warinings\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data visualization and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "#configure\n",
    "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
    "# matplotlib inline\n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid', color_codes=True)\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "\n",
    "#preprocessing\n",
    "from nltk.corpus import stopwords  #stopwords\n",
    "from nltk import word_tokenize, sent_tokenize  # tokenizing\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer  # lammatizer from WordNet\n",
    "\n",
    "# for part-of-speech tagging\n",
    "from nltk import pos_tag\n",
    "\n",
    "# from named entity recognition (NER)\n",
    "from nltk import ne_chunk\n",
    "\n",
    "# vectorizers for creating the document-term-matrix (DTM)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "# BeautifulSoup library\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import re  # regex\n",
    "\n",
    "#model_selection\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#evaluation\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "\n",
    "#prprocssing scikit\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer  # 'Imputer' is deprecated from 'sklearn.preprocessing'\n",
    "\n",
    "#classification.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "\n",
    "#stop-words\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "#keras\n",
    "import keras\n",
    "from keras.preprocessing.text import one_hot, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Embedding, Input, LSTM  # cannot import name 'CuDNNLSTM' from 'keras.layers'\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "#gensim w2v\n",
    "#word2vec\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_frame = pd.read_csv(r'./input/Reviews.csv')\n",
    "df = rev_frame.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 568454 entries, 0 to 568453\n",
      "Data columns (total 10 columns):\n",
      " #   Column                  Non-Null Count   Dtype \n",
      "---  ------                  --------------   ----- \n",
      " 0   Id                      568454 non-null  int64 \n",
      " 1   ProductId               568454 non-null  object\n",
      " 2   UserId                  568454 non-null  object\n",
      " 3   ProfileName             568438 non-null  object\n",
      " 4   HelpfulnessNumerator    568454 non-null  int64 \n",
      " 5   HelpfulnessDenominator  568454 non-null  int64 \n",
      " 6   Score                   568454 non-null  int64 \n",
      " 7   Time                    568454 non-null  int64 \n",
      " 8   Summary                 568427 non-null  object\n",
      " 9   Text                    568454 non-null  object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 43.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserId</th>\n",
       "      <th>ProductId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>#oc-R103C0QSV1DF5E</th>\n",
       "      <th>B006Q820X0</th>\n",
       "      <td>136323</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1343088000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#oc-R109MU5OBBZ59U</th>\n",
       "      <th>B008I1XPKA</th>\n",
       "      <td>516062</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1350086400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#oc-R10LFEMQEW6QGZ</th>\n",
       "      <th>B008I1XPKA</th>\n",
       "      <td>516079</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1345939200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#oc-R10LT57ZGIB140</th>\n",
       "      <th>B0026LJ3EA</th>\n",
       "      <td>378693</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1310601600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#oc-R10UA029WVWIUI</th>\n",
       "      <th>B006Q820X0</th>\n",
       "      <td>136545</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1342483200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZZV9PDNMCOZW</th>\n",
       "      <th>B003SNX4YA</th>\n",
       "      <td>422838</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1329436800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZZVNIMTTMJH6</th>\n",
       "      <th>B000FI4O90</th>\n",
       "      <td>190698</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1268179200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZZY649VYAHQS</th>\n",
       "      <th>B000N9VLJ2</th>\n",
       "      <td>222781</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1309737600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZZYCJOJLUDYR</th>\n",
       "      <th>B001SB22UG</th>\n",
       "      <td>131469</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1337472000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZZZOVIBXHGDR</th>\n",
       "      <th>B001EO5SB2</th>\n",
       "      <td>183897</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1333497600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>560804 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Id  HelpfulnessNumerator  \\\n",
       "UserId             ProductId                                  \n",
       "#oc-R103C0QSV1DF5E B006Q820X0  136323                     1   \n",
       "#oc-R109MU5OBBZ59U B008I1XPKA  516062                     0   \n",
       "#oc-R10LFEMQEW6QGZ B008I1XPKA  516079                     0   \n",
       "#oc-R10LT57ZGIB140 B0026LJ3EA  378693                     0   \n",
       "#oc-R10UA029WVWIUI B006Q820X0  136545                     0   \n",
       "...                               ...                   ...   \n",
       "AZZV9PDNMCOZW      B003SNX4YA  422838                     0   \n",
       "AZZVNIMTTMJH6      B000FI4O90  190698                     0   \n",
       "AZZY649VYAHQS      B000N9VLJ2  222781                     1   \n",
       "AZZYCJOJLUDYR      B001SB22UG  131469                     0   \n",
       "AZZZOVIBXHGDR      B001EO5SB2  183897                     0   \n",
       "\n",
       "                               HelpfulnessDenominator  Score        Time  \n",
       "UserId             ProductId                                              \n",
       "#oc-R103C0QSV1DF5E B006Q820X0                       2      5  1343088000  \n",
       "#oc-R109MU5OBBZ59U B008I1XPKA                       1      5  1350086400  \n",
       "#oc-R10LFEMQEW6QGZ B008I1XPKA                       1      5  1345939200  \n",
       "#oc-R10LT57ZGIB140 B0026LJ3EA                       0      3  1310601600  \n",
       "#oc-R10UA029WVWIUI B006Q820X0                       0      1  1342483200  \n",
       "...                                               ...    ...         ...  \n",
       "AZZV9PDNMCOZW      B003SNX4YA                       0      4  1329436800  \n",
       "AZZVNIMTTMJH6      B000FI4O90                       0      5  1268179200  \n",
       "AZZY649VYAHQS      B000N9VLJ2                       1      5  1309737600  \n",
       "AZZYCJOJLUDYR      B001SB22UG                       0      5  1337472000  \n",
       "AZZZOVIBXHGDR      B001EO5SB2                       0      2  1333497600  \n",
       "\n",
       "[560804 rows x 5 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['UserId', 'ProductId']).sum('Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "939340800\n",
      "1351209600\n"
     ]
    }
   ],
   "source": [
    "print(df['Time'].min())\n",
    "print(df['Time'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256059"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['UserId'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A brief description of the dataset from Overview tab on Kaggle : -\n",
    "\n",
    "Data includes:\n",
    "- Reviews from Oct 1999 - Oct 2012\n",
    "- 568,454 reviews\n",
    "- 256,059 users\n",
    "- 74,258 products\n",
    "- 260 users with > 50 reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA CLEANING AND PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since here I am concerned with **sentiment analysis** I shall keep only the 'Text' and the 'Score' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
       "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Text', 'Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename({'Text': 'review', 'Score': 'rating'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568454, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  rating\n",
       "0  I have bought several of the Vitality canned d...       5\n",
       "1  Product arrived labeled as Jumbo Salted Peanut...       1\n",
       "2  This is a confection that has been around a fe...       4\n",
       "3  If you are looking for the secret ingredient i...       2\n",
       "4  Great taffy at a great price.  There was a wid...       5"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now see if any of the column has any null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for null values\n",
    "print(df['rating'].isnull().sum())\n",
    "df['review'].isnull().sum()  # no null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is no point for keeping rows with different scores or sentiment for same review text.  So I will keep only one instance and drop the rest of the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates/ for every duplicate we will keep only one row of that type. \n",
    "df.drop_duplicates(subset=['review', 'rating'], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(393675, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  rating\n",
       "0  I have bought several of the Vitality canned d...       5\n",
       "1  Product arrived labeled as Jumbo Salted Peanut...       1\n",
       "2  This is a confection that has been around a fe...       4\n",
       "3  If you are looking for the secret ingredient i...       2\n",
       "4  Great taffy at a great price.  There was a wid...       5"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now check the shape. note that shape is reduced which shows that we did has duplicate rows.\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now print some reviews and see if we can get insights from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n",
      "\n",
      "\n",
      "Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".\n",
      "\n",
      "\n",
      "This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.\n",
      "\n",
      "\n",
      "If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.\n",
      "\n",
      "\n",
      "Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printing some reviews to see insights.\n",
    "for review in df['review'][:5]:\n",
    "    print(review + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is nothing much that I can figure out except the fact that there are some stray words and some punctuation that we have to remove before moving ahead.\n",
    "\n",
    "**But note that if I remove the punctuation now then it will be difficult to break the reviews into sentences which is required by Word2Vec constructor in Gensim. So we will first break text into sentences and then clean those sentences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since we are doing sentiment analysis I will convert the values in score column to sentiment. Sentiment is 0 for ratings or scores less than 3 and 1 or  +  elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_sentiment(rating):\n",
    "    if (rating<=3):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['rating'].apply(mark_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['rating'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I have bought several of the Vitality canned d...          1\n",
       "1  Product arrived labeled as Jumbo Salted Peanut...          0\n",
       "2  This is a confection that has been around a fe...          1\n",
       "3  If you are looking for the secret ingredient i...          0\n",
       "4  Great taffy at a great price.  There was a wid...          1"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    306819\n",
       "0     86856\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the sentiment column now has sentiment of the corressponding product review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing steps :\n",
    "\n",
    "1 ) First **removing punctuation and html tags** if any. note that the html tas may be present ast the data must be scraped from net.\n",
    "\n",
    "2) **Tokenize** the reviews into tokens or words .\n",
    "\n",
    "3) Next **remove the stop words and shorter words** as they cause noise.\n",
    "\n",
    "4) **Stem or lemmatize** the words depending on what does better. Herer I have yse lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean and pre-process the text.\n",
    "def clean_reviews(review):\n",
    "    \n",
    "    # 1. Removing html tags\n",
    "    review_text = BeautifulSoup(review, \"lxml\").get_text()\n",
    "    \n",
    "    # 2. Retaining only alphabets.\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)  # 정규표현식\n",
    "    \n",
    "    # 3. Converting to lower case and splitting\n",
    "    word_tokens = review_text.lower().split()\n",
    "    \n",
    "    # 4. Remove stopwords\n",
    "    le=WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = [le.lemmatize(w) for w in word_tokens if w not in stop_words]\n",
    "    \n",
    "    cleaned_review = \" \".join(word_tokens)\n",
    "    \n",
    "    return cleaned_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that pre processing all the reviews is taking way too much time and so I will take only 100K reviews. To balance the class  I have taken equal instances of each sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393675"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = df.loc[df.sentiment==1, :][:50000]\n",
    "neg_df = df.loc[df.sentiment==0, :][:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I got a wild hair for taffy and ordered this f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>This saltwater taffy had great flavors and was...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I have bought several of the Vitality canned d...          1\n",
       "2  This is a confection that has been around a fe...          1\n",
       "4  Great taffy at a great price.  There was a wid...          1\n",
       "5  I got a wild hair for taffy and ordered this f...          1\n",
       "6  This saltwater taffy had great flavors and was...          1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>My cats have been happily eating Felidae Plati...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I love eating them and they are good for watch...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>The candy is just red , No flavor . Just  plan...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review  sentiment\n",
       "1   Product arrived labeled as Jumbo Salted Peanut...          0\n",
       "3   If you are looking for the secret ingredient i...          0\n",
       "12  My cats have been happily eating Felidae Plati...          0\n",
       "16  I love eating them and they are good for watch...          0\n",
       "26  The candy is just red , No flavor . Just  plan...          0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now combine reviews of each sentiment and shuffle them so that their order doesn't make any sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining\n",
    "df = pd.concat([pos_df, neg_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I got a wild hair for taffy and ordered this f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This saltwater taffy had great flavors and was...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I have bought several of the Vitality canned d...          1\n",
       "1  This is a confection that has been around a fe...          1\n",
       "2  Great taffy at a great price.  There was a wid...          1\n",
       "3  I got a wild hair for taffy and ordered this f...          1\n",
       "4  This saltwater taffy had great flavors and was...          1"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I really wish Amazon would list ingredients fo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Green Mountain Coffee - Dark Magic &amp; Double Bl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I admit to being one of those annoying self-co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I am very disappointed that Gerber has added D...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>taste just like pumpkin pie. not too sweet.&lt;br...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I really wish Amazon would list ingredients fo...          0\n",
       "1  Green Mountain Coffee - Dark Magic & Double Bl...          1\n",
       "2  I admit to being one of those annoying self-co...          1\n",
       "3  I am very disappointed that Gerber has added D...          0\n",
       "4  taste just like pumpkin pie. not too sweet.<br...          1"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffling rows\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING GOOGLE WORD2VEC WORD EMBEDDINGS IN GENSIM\n",
    "\n",
    "In this section I have actually created the word embeddings in Gensim. Note that I planed touse the pre-trained word embeddings like the google word2vec trained on google news corpusor the famous Stanford Glove embeddings. But as soon as I load the corressponding embeddings through Gensim the runtime dies and kernel crashes ; perhaps because it contains 30L words and which is exceeding the RAM on Google Colab.\n",
    "\n",
    "Because of this ; for now I have created the embeddings by training on my own corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim\n",
    "# # load Google's pre-trained Word2Vec model.\n",
    "# pre_w2v_model = gensim.models.KeyedVectors.load_word2vec_format(r'drive/Colab Notebooks/amazon food reviews/GoogleNews-vectors-negative300.bin', binary=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to break our data into sentences which is requires by the constructor of the Word2Vec class in Gensim. For this I have used Punk English tokenizer from the NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I really wish Amazon would list ingredients fo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Green Mountain Coffee - Dark Magic &amp; Double Bl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I admit to being one of those annoying self-co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I am very disappointed that Gerber has added D...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>taste just like pumpkin pie. not too sweet.&lt;br...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I really wish Amazon would list ingredients fo...          0\n",
       "1  Green Mountain Coffee - Dark Magic & Double Bl...          1\n",
       "2  I admit to being one of those annoying self-co...          1\n",
       "3  I am very disappointed that Gerber has added D...          0\n",
       "4  taste just like pumpkin pie. not too sweet.<br...          1"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [02:33<00:00, 650.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum:  512639\n",
      "512639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentences = []\n",
    "sum = 0\n",
    "\n",
    "for review in tqdm(df['review']):\n",
    "    sents = tokenizer.tokenize(review.strip())\n",
    "    sum += len(sents)\n",
    "    for sent in sents:\n",
    "        cleaned_sent = clean_reviews(sent)  # text 전처리\n",
    "        sentences.append(cleaned_sent.split())  # can user word_tokenizer also.\n",
    "print(\"sum: \", sum)\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us print some sentences just to check iff they are in the correct fornat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['really', 'wish', 'amazon', 'would', 'list', 'ingredient', 'food', 'product', 'sell', 'whether', 'human', 'animal', 'consumption']\n",
      "['reason', 'assumed', 'greenies', 'natural', 'completely', 'fault', 'looking', 'ingredient', 'another', 'site', 'buying', 'yeah', 'corn', 'based', 'thus', 'different', 'junk', 'get', 'supermarket']\n",
      "['oh', 'except', 'cost']\n",
      "['buy']\n",
      "['sure', 'cat', 'like', 'seems', 'like', 'treat', 'give', 'plus', 'simply', 'liking', 'make', 'good', 'healthy', 'much', 'like', 'inhaling', 'box', 'cheez', 'deliciously', 'naughty', 'list', 'ingredient', 'salmon', 'greenies', 'anyone', 'care', 'chicken', 'meal', 'ground', 'brewer', 'rice', 'ground', 'wheat', 'corn', 'gluten', 'meal', 'poultry', 'fat', 'preserved', 'mixed', 'tocopherol', 'sprayed', 'dried', 'hydrolyzed', 'chicken', 'protein', 'concentrate', 'oat', 'fiber', 'salmon', 'meal', 'natural', 'chicken', 'flavor', 'vegetable', 'oil', 'preserved', 'mixed', 'tocopherol', 'natural', 'poultry', 'fish', 'flavor', 'sodium', 'gluconate', 'brewer', 'dried', 'yeast', 'calcium', 'carbonate', 'potassium', 'chloride', 'glucono', 'delta', 'lactone', 'citric', 'acid', 'choline', 'chloride', 'taurine', 'vitamin', 'vitamin', 'supplement', 'activated', 'animal', 'sterol', 'source', 'vitamin', 'supplement', 'dl', 'alpha', 'tocopherol', 'acetate', 'source', 'vitamin', 'e', 'niacin', 'supplement', 'calcium', 'pantothenate', 'thiamine', 'mononitrate', 'vitamin', 'b', 'pyridoxine', 'hydrochloride', 'vitamin', 'b', 'riboflavin', 'supplement', 'menadione', 'sodium', 'bisulfite', 'complex', 'source', 'vitamin', 'k', 'activity', 'folic', 'acid', 'biotin', 'vitamin', 'b', 'supplement', 'mineral', 'zinc', 'proteinate', 'copper', 'proteinate', 'manganese', 'proteinate', 'iron', 'sulfate', 'sodium', 'selenite', 'cobalt', 'carbonate', 'ethylenediamine', 'dihydriodide', 'mixed', 'tocopherol', 'rosemary', 'extract', 'dl', 'methionine', 'chlorophyll']\n"
     ]
    }
   ],
   "source": [
    "# trying to print few sentences\n",
    "for te in sentences[:5]:\n",
    "    print(te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now actually creating the word 2 vec embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "w2v_model = gensim.models.Word2Vec(sentences=sentences, size=300, window=10, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters:\n",
    "\n",
    "- **sentences :** The sentences we have obtained.\n",
    "- **size :** The dimesnions of the vector used to represent each word.\n",
    "- **window :** The number f words around any word to see the context.\n",
    "- **min_count :** The minimum number of times a word should appear for its embedding to be formed or learnt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38408827, 41193730)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.train(sentences, epochs=10, total_examples=len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now can try some things with word2vec embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.83688974e-01,  5.70252955e-01,  1.79976702e+00, -1.00749405e-02,\n",
       "       -9.56964314e-01, -5.00047088e-01, -6.18228137e-01, -4.70298618e-01,\n",
       "       -1.31175053e+00,  2.45180416e+00,  5.45927644e-01,  6.46868765e-01,\n",
       "       -4.49483216e-01, -1.03179228e+00,  1.57077420e+00,  1.43029165e+00,\n",
       "        1.00573264e-01,  6.64683223e-01, -6.85839057e-01, -1.96350098e-01,\n",
       "        6.33605480e-01,  6.77996725e-02, -6.09865189e-01, -1.07628977e+00,\n",
       "        4.55098860e-02,  1.51399195e-01,  1.00566551e-01,  4.40805480e-02,\n",
       "        9.97650683e-01, -9.86560509e-02, -4.15108979e-01, -1.03947461e-01,\n",
       "       -1.05583215e+00,  7.63348401e-01, -6.99360907e-01, -1.33468226e-01,\n",
       "        2.30404034e-01, -6.94058061e-01, -8.50645304e-01, -8.54234457e-01,\n",
       "       -2.47394174e-01,  1.05335310e-01, -1.21529090e+00,  3.91671667e-03,\n",
       "       -6.55919671e-01,  1.52958243e-03, -1.53237998e+00, -7.91378438e-01,\n",
       "       -1.38934505e+00,  9.80016232e-01, -1.60336435e+00, -9.32592034e-01,\n",
       "        3.62756640e-01,  5.22699773e-01,  8.50506201e-02,  3.11784893e-02,\n",
       "        1.34979284e+00, -7.53154933e-01, -5.99547684e-01,  2.65056968e-01,\n",
       "       -2.19297230e-01,  2.86805958e-01, -1.22837842e+00, -5.50392270e-01,\n",
       "       -6.87567711e-01,  9.88881707e-01,  5.53472459e-01, -2.59249389e-01,\n",
       "        1.83054239e-01,  2.54734844e-01, -4.99039531e-01,  9.46907520e-01,\n",
       "       -4.27992851e-01, -2.41669163e-01,  2.69728392e-01, -1.13726330e+00,\n",
       "       -9.18933749e-01, -4.33824122e-01,  1.00130486e+00, -1.25345898e+00,\n",
       "        8.26608956e-01, -2.52212167e-01, -2.51980305e-01,  5.15348077e-01,\n",
       "        4.81649965e-01, -1.33417651e-01,  7.91563451e-01,  2.61876702e-01,\n",
       "       -7.38808513e-02,  1.33168793e+00, -1.06213427e+00, -7.91922688e-01,\n",
       "       -3.99359614e-01, -7.53880441e-01, -1.69252977e-01,  1.58931398e+00,\n",
       "        5.72085321e-01,  4.33827281e-01,  4.41293210e-01,  1.08841991e+00,\n",
       "       -7.78533280e-01,  8.80500972e-01,  6.21223524e-02, -6.67839311e-04,\n",
       "       -1.02853858e+00, -1.42595255e+00, -6.15798652e-01,  9.79076326e-01,\n",
       "       -1.09789299e-03, -2.35718787e-01,  9.89969492e-01, -1.04483388e-01,\n",
       "       -1.46023846e+00,  4.51659918e-01, -1.03584599e+00, -4.96672183e-01,\n",
       "       -7.59648800e-01,  7.82533288e-01, -4.67171520e-01,  5.52941799e-01,\n",
       "        3.76638807e-02, -3.69206518e-01, -6.37449563e-01,  1.11735724e-01,\n",
       "        1.75653410e+00,  2.92149663e-01, -3.38516116e-01, -5.71258247e-01,\n",
       "        2.70192057e-01,  9.85586882e-01,  5.86756229e-01,  3.99585903e-01,\n",
       "       -1.26367852e-01, -7.46239781e-01, -9.16279018e-01, -8.19507122e-01,\n",
       "        9.08470333e-01, -1.01514173e+00, -3.76309961e-01,  3.89770985e-01,\n",
       "        1.74994743e+00, -2.02121556e-01,  1.86327970e+00,  9.26995516e-01,\n",
       "        6.57496810e-01, -2.50007689e-01,  3.92125666e-01,  9.19692934e-01,\n",
       "       -2.53609347e+00, -1.98895652e-02, -1.26767325e+00, -1.38011765e+00,\n",
       "       -2.09479570e-01, -4.49910993e-03,  1.62267238e-01,  9.34574679e-02,\n",
       "        6.86674535e-01, -3.98950964e-01, -6.01805151e-01,  8.02110322e-03,\n",
       "       -5.08378446e-01, -7.45870650e-01, -9.77780461e-01, -6.30165517e-01,\n",
       "        4.41786140e-01,  1.37322217e-01,  5.30099034e-01, -4.89256606e-02,\n",
       "        5.98319173e-01, -1.32975161e-01, -2.76529074e-01, -6.72676146e-01,\n",
       "        1.07597673e+00,  1.35565281e+00,  8.65606740e-02,  1.03407621e+00,\n",
       "       -1.35352567e-01, -5.42354703e-01, -2.50170171e-01, -1.21248519e+00,\n",
       "        1.89094162e+00,  5.14801800e-01, -1.24331641e+00,  5.37913859e-01,\n",
       "       -1.25331819e+00, -5.25841534e-01,  2.42063135e-01, -1.14740288e+00,\n",
       "       -2.47889031e-02, -4.43995088e-01, -8.83153617e-01, -9.21844363e-01,\n",
       "        2.04533860e-01,  2.74753124e-01,  6.08265460e-01, -7.74649620e-01,\n",
       "       -2.32585475e-01, -3.23455073e-02, -4.50841904e-01, -2.38694340e-01,\n",
       "        3.10110837e-01,  8.53805393e-02,  3.45774472e-01,  2.37709552e-01,\n",
       "        1.16352201e-01,  6.15338624e-01, -1.82847351e-01, -3.02350909e-01,\n",
       "        7.84202158e-01,  4.98629034e-01, -1.63354003e+00,  2.04854107e+00,\n",
       "       -1.35091737e-01, -8.31996202e-01, -9.00626361e-01, -7.49387801e-01,\n",
       "        6.54051840e-01,  3.63124728e-01, -7.52253056e-01,  4.53544825e-01,\n",
       "        8.46839428e-01,  1.05305004e+00,  4.24557447e-01,  4.18956518e-01,\n",
       "       -4.33434367e-01,  2.18854523e+00,  2.01616332e-01,  6.30538315e-02,\n",
       "        5.91308653e-01,  1.37559980e-01, -6.87335610e-01, -3.64124119e-01,\n",
       "        6.05102777e-02, -4.48929399e-01,  1.07168055e+00, -4.19102311e-01,\n",
       "       -6.12122834e-01, -8.43887180e-02,  5.57711899e-01, -5.09598255e-01,\n",
       "       -3.83323491e-01,  7.29730446e-03, -6.94388926e-01,  3.40814948e-01,\n",
       "        6.55628324e-01, -8.05592090e-02, -9.85066872e-03,  3.28968912e-01,\n",
       "       -1.53503275e+00,  1.09141719e+00,  1.35876596e-01,  2.42442161e-01,\n",
       "       -3.06115746e-01, -1.42027247e+00, -6.44427240e-01, -9.94240880e-01,\n",
       "        3.55515420e-01,  4.94824760e-02,  5.97630560e-01, -7.12575495e-01,\n",
       "       -1.14745237e-01,  7.92880058e-02, -1.03095007e+00, -8.29927266e-01,\n",
       "        8.29595923e-01, -1.10643730e-02, -2.11834878e-01,  7.72438824e-01,\n",
       "       -1.71112466e+00,  5.61873376e-01, -8.25199842e-01,  2.92037278e-01,\n",
       "        7.42428064e-01,  2.96442777e-01, -9.07881781e-02, -3.38227954e-03,\n",
       "        3.46026957e-01, -1.63026416e+00,  4.89897132e-01, -7.07613468e-01,\n",
       "       -9.06381190e-01,  3.46704334e-01, -2.16569257e+00,  9.34597433e-01,\n",
       "       -5.10805488e-01, -4.28067118e-01,  3.13255817e-01,  8.69766533e-01,\n",
       "        4.45372403e-01,  9.13057625e-01,  4.05258119e-01, -1.84736233e-02,\n",
       "       -1.57618713e+00,  5.63848078e-01,  5.65852880e-01,  1.43057248e-02,\n",
       "       -8.71719658e-01, -1.21092349e-01, -4.06157859e-02, -5.93440473e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding of a particular word.\n",
    "w2v_model.wv.get_vector('like')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words are :  56379\n"
     ]
    }
   ],
   "source": [
    "# total number of extracted words.\n",
    "vocab = w2v_model.wv.vocab\n",
    "print(\"The total number of words are : \",len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reminded', 0.49845317006111145),\n",
       " ('weird', 0.4922550618648529),\n",
       " ('alright', 0.48545411229133606),\n",
       " ('strange', 0.46986639499664307),\n",
       " ('reminds', 0.45902395248413086),\n",
       " ('akin', 0.44934549927711487),\n",
       " ('funny', 0.4454163908958435),\n",
       " ('reminiscent', 0.42602288722991943),\n",
       " ('gross', 0.42208850383758545),\n",
       " ('remind', 0.41538625955581665)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words most similar to a given word.\n",
    "w2v_model.wv.most_similar('like')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3864205"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similaraity b/w two words\n",
    "w2v_model.wv.similarity('good','like')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now creating a dictionary with words in vocab and their embeddings. This will be used when we will be creating embedding matrix (for feeding to keras embedding layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The no of words : 56379\n"
     ]
    }
   ],
   "source": [
    "print(\"The no of words :\",len(vocab))\n",
    "# print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vocab)\n",
    "vocab = list(vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The no of key-value pairs :  56379\n"
     ]
    }
   ],
   "source": [
    "word_vec_dict = {}\n",
    "for word in vocab:\n",
    "    word_vec_dict[word] = w2v_model.wv.get_vector(word)\n",
    "print(\"The no of key-value pairs : \", len(word_vec_dict)) # should come equal to vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # just check\n",
    "# for word in vocab[:5]:\n",
    "#   print(word_vec_dict[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPARING THE DATA FOR KERAS EMBEDDING LAYER.\n",
    "\n",
    "Now we have obtained the w2v embeddings. But there are a couple of steps required by Keras embedding layer before we can move on.\n",
    "\n",
    "**Also note that since w2v embeddings have been made now ; we can preprocess our review column by using the function that we saw above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning reviews.\n",
    "df['clean_review'] = df['review'].apply(clean_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find the maximum lenght of any document or review in our case. WE will pad all reviews to have this same length.This will be required by Keras embedding layer. Must check [this](https://www.kaggle.com/rajmehra03/a-detailed-explanation-of-keras-embedding-layer) kernel on Kaggle for a wonderful explanation of keras embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I really wish Amazon would list ingredients fo...</td>\n",
       "      <td>0</td>\n",
       "      <td>really wish amazon would list ingredient food ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Green Mountain Coffee - Dark Magic &amp; Double Bl...</td>\n",
       "      <td>1</td>\n",
       "      <td>green mountain coffee dark magic double black ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I admit to being one of those annoying self-co...</td>\n",
       "      <td>1</td>\n",
       "      <td>admit one annoying self confessed coffee snob ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I am very disappointed that Gerber has added D...</td>\n",
       "      <td>0</td>\n",
       "      <td>disappointed gerber added dha green bean son u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>taste just like pumpkin pie. not too sweet.&lt;br...</td>\n",
       "      <td>1</td>\n",
       "      <td>taste like pumpkin pie sweet expensive placing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment  \\\n",
       "0  I really wish Amazon would list ingredients fo...          0   \n",
       "1  Green Mountain Coffee - Dark Magic & Double Bl...          1   \n",
       "2  I admit to being one of those annoying self-co...          1   \n",
       "3  I am very disappointed that Gerber has added D...          0   \n",
       "4  taste just like pumpkin pie. not too sweet.<br...          1   \n",
       "\n",
       "                                        clean_review  \n",
       "0  really wish amazon would list ingredient food ...  \n",
       "1  green mountain coffee dark magic double black ...  \n",
       "2  admit one annoying self confessed coffee snob ...  \n",
       "3  disappointed gerber added dha green bean son u...  \n",
       "4  taste like pumpkin pie sweet expensive placing...  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1564\n"
     ]
    }
   ],
   "source": [
    "# number of unique words = 56379\n",
    "# now since we will have to pad we need to find the maximum length of any document.\n",
    "\n",
    "maxi = -1\n",
    "\n",
    "for i, rev in enumerate(df['clean_review']):\n",
    "    tokens = rev.split()\n",
    "    if (len(tokens) > maxi):\n",
    "        maxi = len(tokens)\n",
    "print(maxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we integer encode the words in the reviews using Keras tokenizer. \n",
    "\n",
    "**Note that there two important variables: which are the vocab_size which is the total no of unique words while the second is max_doc_len which is the length of every document after padding. Both of these are required by the Keras embedding layer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(df['clean_review'])\n",
    "vocab_size = len(tok.word_index) + 1\n",
    "encd_rev = tok.texts_to_sequences(df['clean_review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rev_len = 1565  # max length of a review\n",
    "vocab_size = len(tok.word_index) + 1  # total no of words\n",
    "embed_dim = 300  #  embedding dimension as choosen in word2vec constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 1565)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now padding to have a maximum lenght of 1565\n",
    "pad_rev = pad_sequences(encd_rev, maxlen=max_rev_len, padding='post')\n",
    "pad_rev.shape  # note that we had 100k reviews and we gave padded each review to have a length of 1565 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING THE EMBEDDING MATRIX\n",
    "\n",
    "#### Now we need to pass the w2v word embeddings to the embedding layer in Keras. For this we will create the embedding matrix and pass it as 'embedding_initializer' parameter to the layer.\n",
    "\n",
    "**The embedding matrix will be of dimensions (vocab_size,embed_dim) where the word_index of each word from keras tokenizer is its index into the matrix and the corressponding entry is its w2v vector ;)**\n",
    "\n",
    "**Note that there may be words which will not be present in embeddings learnt by the w2v model. The embedding matrix entry corressponding to those words will be a vector of all zeros.**\n",
    "\n",
    "**Also note that if u are thinkng why won't a word be present then it is bcoz now we have learnt on out own corpus but if we use pre-trained embedding then it may happen that some words specific to our dataset aren't present then in those cases we may use a fixed vector of zeros to denote all those words that earen;t present in th pre-trained embeddings. Also note that it may also happen that some words are not present ifu have filtered some words by setting min_count in w2v constructor.\n",
    "  **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now creating the embedding matrix\n",
    "embed_matrix = np.zeros(shape=(vocab_size, embed_dim))\n",
    "for word, i in tok.word_index.items():\n",
    "    embed_vector = word_vec_dict.get(word)\n",
    "    if embed_vector is not None:  # word is in the vocabulary learned by the w2v model\n",
    "        embed_matrix[i] = embed_vector\n",
    "    # if word is not found the embed_vector corressponding to that vector will stay zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.83688974e-01  5.70252955e-01  1.79976702e+00 -1.00749405e-02\n",
      " -9.56964314e-01 -5.00047088e-01 -6.18228137e-01 -4.70298618e-01\n",
      " -1.31175053e+00  2.45180416e+00  5.45927644e-01  6.46868765e-01\n",
      " -4.49483216e-01 -1.03179228e+00  1.57077420e+00  1.43029165e+00\n",
      "  1.00573264e-01  6.64683223e-01 -6.85839057e-01 -1.96350098e-01\n",
      "  6.33605480e-01  6.77996725e-02 -6.09865189e-01 -1.07628977e+00\n",
      "  4.55098860e-02  1.51399195e-01  1.00566551e-01  4.40805480e-02\n",
      "  9.97650683e-01 -9.86560509e-02 -4.15108979e-01 -1.03947461e-01\n",
      " -1.05583215e+00  7.63348401e-01 -6.99360907e-01 -1.33468226e-01\n",
      "  2.30404034e-01 -6.94058061e-01 -8.50645304e-01 -8.54234457e-01\n",
      " -2.47394174e-01  1.05335310e-01 -1.21529090e+00  3.91671667e-03\n",
      " -6.55919671e-01  1.52958243e-03 -1.53237998e+00 -7.91378438e-01\n",
      " -1.38934505e+00  9.80016232e-01 -1.60336435e+00 -9.32592034e-01\n",
      "  3.62756640e-01  5.22699773e-01  8.50506201e-02  3.11784893e-02\n",
      "  1.34979284e+00 -7.53154933e-01 -5.99547684e-01  2.65056968e-01\n",
      " -2.19297230e-01  2.86805958e-01 -1.22837842e+00 -5.50392270e-01\n",
      " -6.87567711e-01  9.88881707e-01  5.53472459e-01 -2.59249389e-01\n",
      "  1.83054239e-01  2.54734844e-01 -4.99039531e-01  9.46907520e-01\n",
      " -4.27992851e-01 -2.41669163e-01  2.69728392e-01 -1.13726330e+00\n",
      " -9.18933749e-01 -4.33824122e-01  1.00130486e+00 -1.25345898e+00\n",
      "  8.26608956e-01 -2.52212167e-01 -2.51980305e-01  5.15348077e-01\n",
      "  4.81649965e-01 -1.33417651e-01  7.91563451e-01  2.61876702e-01\n",
      " -7.38808513e-02  1.33168793e+00 -1.06213427e+00 -7.91922688e-01\n",
      " -3.99359614e-01 -7.53880441e-01 -1.69252977e-01  1.58931398e+00\n",
      "  5.72085321e-01  4.33827281e-01  4.41293210e-01  1.08841991e+00\n",
      " -7.78533280e-01  8.80500972e-01  6.21223524e-02 -6.67839311e-04\n",
      " -1.02853858e+00 -1.42595255e+00 -6.15798652e-01  9.79076326e-01\n",
      " -1.09789299e-03 -2.35718787e-01  9.89969492e-01 -1.04483388e-01\n",
      " -1.46023846e+00  4.51659918e-01 -1.03584599e+00 -4.96672183e-01\n",
      " -7.59648800e-01  7.82533288e-01 -4.67171520e-01  5.52941799e-01\n",
      "  3.76638807e-02 -3.69206518e-01 -6.37449563e-01  1.11735724e-01\n",
      "  1.75653410e+00  2.92149663e-01 -3.38516116e-01 -5.71258247e-01\n",
      "  2.70192057e-01  9.85586882e-01  5.86756229e-01  3.99585903e-01\n",
      " -1.26367852e-01 -7.46239781e-01 -9.16279018e-01 -8.19507122e-01\n",
      "  9.08470333e-01 -1.01514173e+00 -3.76309961e-01  3.89770985e-01\n",
      "  1.74994743e+00 -2.02121556e-01  1.86327970e+00  9.26995516e-01\n",
      "  6.57496810e-01 -2.50007689e-01  3.92125666e-01  9.19692934e-01\n",
      " -2.53609347e+00 -1.98895652e-02 -1.26767325e+00 -1.38011765e+00\n",
      " -2.09479570e-01 -4.49910993e-03  1.62267238e-01  9.34574679e-02\n",
      "  6.86674535e-01 -3.98950964e-01 -6.01805151e-01  8.02110322e-03\n",
      " -5.08378446e-01 -7.45870650e-01 -9.77780461e-01 -6.30165517e-01\n",
      "  4.41786140e-01  1.37322217e-01  5.30099034e-01 -4.89256606e-02\n",
      "  5.98319173e-01 -1.32975161e-01 -2.76529074e-01 -6.72676146e-01\n",
      "  1.07597673e+00  1.35565281e+00  8.65606740e-02  1.03407621e+00\n",
      " -1.35352567e-01 -5.42354703e-01 -2.50170171e-01 -1.21248519e+00\n",
      "  1.89094162e+00  5.14801800e-01 -1.24331641e+00  5.37913859e-01\n",
      " -1.25331819e+00 -5.25841534e-01  2.42063135e-01 -1.14740288e+00\n",
      " -2.47889031e-02 -4.43995088e-01 -8.83153617e-01 -9.21844363e-01\n",
      "  2.04533860e-01  2.74753124e-01  6.08265460e-01 -7.74649620e-01\n",
      " -2.32585475e-01 -3.23455073e-02 -4.50841904e-01 -2.38694340e-01\n",
      "  3.10110837e-01  8.53805393e-02  3.45774472e-01  2.37709552e-01\n",
      "  1.16352201e-01  6.15338624e-01 -1.82847351e-01 -3.02350909e-01\n",
      "  7.84202158e-01  4.98629034e-01 -1.63354003e+00  2.04854107e+00\n",
      " -1.35091737e-01 -8.31996202e-01 -9.00626361e-01 -7.49387801e-01\n",
      "  6.54051840e-01  3.63124728e-01 -7.52253056e-01  4.53544825e-01\n",
      "  8.46839428e-01  1.05305004e+00  4.24557447e-01  4.18956518e-01\n",
      " -4.33434367e-01  2.18854523e+00  2.01616332e-01  6.30538315e-02\n",
      "  5.91308653e-01  1.37559980e-01 -6.87335610e-01 -3.64124119e-01\n",
      "  6.05102777e-02 -4.48929399e-01  1.07168055e+00 -4.19102311e-01\n",
      " -6.12122834e-01 -8.43887180e-02  5.57711899e-01 -5.09598255e-01\n",
      " -3.83323491e-01  7.29730446e-03 -6.94388926e-01  3.40814948e-01\n",
      "  6.55628324e-01 -8.05592090e-02 -9.85066872e-03  3.28968912e-01\n",
      " -1.53503275e+00  1.09141719e+00  1.35876596e-01  2.42442161e-01\n",
      " -3.06115746e-01 -1.42027247e+00 -6.44427240e-01 -9.94240880e-01\n",
      "  3.55515420e-01  4.94824760e-02  5.97630560e-01 -7.12575495e-01\n",
      " -1.14745237e-01  7.92880058e-02 -1.03095007e+00 -8.29927266e-01\n",
      "  8.29595923e-01 -1.10643730e-02 -2.11834878e-01  7.72438824e-01\n",
      " -1.71112466e+00  5.61873376e-01 -8.25199842e-01  2.92037278e-01\n",
      "  7.42428064e-01  2.96442777e-01 -9.07881781e-02 -3.38227954e-03\n",
      "  3.46026957e-01 -1.63026416e+00  4.89897132e-01 -7.07613468e-01\n",
      " -9.06381190e-01  3.46704334e-01 -2.16569257e+00  9.34597433e-01\n",
      " -5.10805488e-01 -4.28067118e-01  3.13255817e-01  8.69766533e-01\n",
      "  4.45372403e-01  9.13057625e-01  4.05258119e-01 -1.84736233e-02\n",
      " -1.57618713e+00  5.63848078e-01  5.65852880e-01  1.43057248e-02\n",
      " -8.71719658e-01 -1.21092349e-01 -4.06157859e-02 -5.93440473e-01]\n"
     ]
    }
   ],
   "source": [
    "# checking\n",
    "print(embed_matrix[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPARING TRAIN AND VALIDATION SETS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepraed train and test sets first\n",
    "Y = keras.utils.to_categorical(df['sentiment'])  # one hot target as required by NN.\n",
    "x_train, x_test, y_train, y_test = train_test_split(pad_rev, Y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUILDING A MODEL AND FINALLY PERFORMING TEXT CLASSIFICATION\n",
    "\n",
    "Having done all the pre-requisites we finally move onto make model in Keras .\n",
    "\n",
    "**Note that I have commented the LSTM layer as including it causes the trainig loss to be stucked at a value of about 0.6932. I don;t know why ;(.**\n",
    "\n",
    "**In case someone knows please comment below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import Constant\n",
    "from keras.layers import ReLU\n",
    "from keras.layers import Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size,\n",
    "                   output_dim=embed_dim,\n",
    "                   input_length=max_rev_len,\n",
    "                  embeddings_initializer=Constant(embed_matrix)))\n",
    "\n",
    "# model.add(CuDNNLSTM(64,return_sequences=False)) # loss stucks at about \n",
    "model.add(Flatten())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.50))\n",
    "# model.add(Dense(16,activation='relu'))\n",
    "# model.add(Dropout(0.20))\n",
    "model.add(Dense(2, activation='sigmoid'))  # sigmoid for bin. classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now print a summary of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comile the model\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(lr=1e-3), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify batch size and epochs for training.\n",
    "epochs=5\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1250/1250 [==============================] - 210s 168ms/step - loss: 0.5324 - accuracy: 0.7397 - val_loss: 0.4205 - val_accuracy: 0.8166\n",
      "Epoch 2/5\n",
      "1250/1250 [==============================] - 233s 186ms/step - loss: 0.4333 - accuracy: 0.7980 - val_loss: 0.3909 - val_accuracy: 0.8303\n",
      "Epoch 3/5\n",
      "1250/1250 [==============================] - 220s 176ms/step - loss: 0.3891 - accuracy: 0.8234 - val_loss: 0.3833 - val_accuracy: 0.8404\n",
      "Epoch 4/5\n",
      "1250/1250 [==============================] - 207s 166ms/step - loss: 0.3476 - accuracy: 0.8461 - val_loss: 0.3926 - val_accuracy: 0.8393\n",
      "Epoch 5/5\n",
      "1250/1250 [==============================] - 202s 161ms/step - loss: 0.3199 - accuracy: 0.8598 - val_loss: 0.4137 - val_accuracy: 0.8424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0cd05fc090>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the model.\n",
    "model.fit(x_train, \n",
    "          y_train, \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size,\n",
    "          validation_data=(x_test, y_test)\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that loss as well as val_loss is  is still deceasing. You can train for more no of epochs but I am not so patient ;)\n",
    "\n",
    "**The final accuracy after 5 epochs is about 84% which is pretty decent.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FURTHER IDEAS :\n",
    "\n",
    "1) ProductId and UserId can be used to track the general ratings of a given product and also to track the review patter of a particular user as if he is strict in reviwing or not.\n",
    " \n",
    "\n",
    "2) Helpfulness feature may tell about the product. This is because gretare the no of people talking about reviews, the mre stronger or critical it is expected to be.\n",
    "\n",
    "3) Summary column can also give a hint.\n",
    "\n",
    "4) One can also try the pre-trained embeddings like Glove word vectors etc...\n",
    "\n",
    "5) Lastly tuning the n/w hyperparameters is always an option;).\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jipiration",
   "language": "python",
   "name": "jipiration"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
